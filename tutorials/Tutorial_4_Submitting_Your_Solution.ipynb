{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial 4** - Submitting Your Solution\n",
    "\n",
    "You learned about AI agents in the previous tutorial and about the data in tutorial number 2. Now, it's time to combine this knowledge into one working solution! \n",
    "Unlike in previous GDSC where you were just asked to hand in the results of your model, this year we ask you to submit running `source code`.\n",
    "I.e. you will create a full blown Chatbot API. We will then run your code (What could possibly go wrong?), and ask it to answer a set of questions.\n",
    "The answers to these questions will be stored in a database and used in the [Chatbot Arena](https://gdsc.ce.capgemini.com/app/arena/) where you (and everybody else) will vote for the best solution!\n",
    "\n",
    "In this tutorial, we will work with the `BasicPIRLSCrew` from [tutorial 3](https://github.com/cg-gdsc/GDSC-7/blob/main/tutorials/Tutorial_3_Introduction_to_AI_Agents.ipynb). You will learn how to turn our Juypter Notebook solution into a real chatbot, how to test your code both locally and remotely using AWS, and how to submit your solution to compete with other teams.\n",
    "\n",
    "But this is not all! As part of this year's GDSC, you are also required to evaluate the submissions of other teams. At the end of this tutorial, we will explain why your role as evaluators is essential and how you can participate in the [Chatbot Arena](https://gdsc.ce.capgemini.com/app/arena/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "1. [Understanding the project structure](#understanding-the-project-structure) - explains the reasoning behid the code structure and which files are crutial for your submission.\n",
    "2. [Create your first application](#create-your-first-application) - covers the steps to run your first app.\n",
    "3. [How to test your code](#how-to-test-your-code) - shows you how to run tests both locally and remotely.\n",
    "4. [How to submit your code](#how-to-submit-your-code) - shows you how to finally submit your solution to the competition.\n",
    "5. [How does the evaluation work](#how-does-the-evaluation-work) - explains the logic of automatic evaluation.\n",
    "6. [How to check the status of your application](#how-to-check-the-status-of-your-application) - access the logs.\n",
    "7. [What to tdo if the automatic evaluation fails](#what-to-do-if-the-automatic-evaluation-fails)\n",
    "8. [Chatbot Arena](#chatbot-arena) - shows you how to use the Arena and explains why it is important to rate battles.\n",
    "9. [Human evaluation questions](#human-evaluation-questions) - shows you how to add new questions to the competition and explains the benefits of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the project structure\n",
    "\n",
    "Before we get to the code, let's take a step back and review the code structure:\n",
    "\n",
    "## [Code Commit](https://aws.amazon.com/de/codecommit/)\n",
    "When you created or joined a team, you gained access to an AWS account. From the AWS Management Console, you can navigate to AWS CodeCommit to view your team's repository.\n",
    "\n",
    "1. [<img src=\"../images/t4_code_commit_1.png\" width=800/>](../images/t4_code_commit_1.png) $\\space\\space\\space\\space$ 2. [<img src=\"../images/t4_code_commit_2.png\" width=800/>](../images/t4_code_commit_2.png)\n",
    "\n",
    "In CodeCommit, you can see your team's repository with code that has already been prepared for you. We will explain this code in more detail throughout this and the next tutorial.\n",
    "\n",
    "## Code structure\n",
    "\n",
    "In your [CodeCommit repository](https://aws.amazon.com/de/codecommit/), that you should also see on the left hand side of this notebook, you will find three folders:\n",
    "\n",
    "- `images` - Containing all the images used in the tutorials\n",
    "- `src` - Containing the source code you will actually submit\n",
    "- `tutorials` - The trainings that you are currently going through\n",
    "\n",
    "In the `src` folder, there are two main directories: `submission` and `static`.\n",
    "\n",
    "- `submission` - **This is the directory where all of your code will be placed**. Here, you can modify and create new crews, tools, agents, and do whatever your heart desires.\n",
    "- `static` - This directory contains code that cannot be modified. The static directory is replaced by the GDSC team with each submission, so no changes made here will be reflected in your final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the considerable freedom given to participants in the submission directory, there is one particularly important file to which you must pay extra attention:  [src/submission/create_submission.py](../src/submission/create_submission.py). Let's take a look inside!\n",
    "\n",
    "```python\n",
    "from src.static.ChatBedrockWrapper import ChatBedrockWrapper\n",
    "from src.static.submission import Submission\n",
    "\n",
    "def create_submission(call_id: str) -> Submission:\n",
    "    ...\n",
    "```\n",
    "\n",
    "As you can see, there's a single function defined. This function will be **the entry point** for your submission. It is used to instantiate your submission, and the signature of this function cannot be modified. It must take a string named `call_id` and return an object of type `Submission`. The body of the function and necessary imports are up to your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is `Submission`\n",
    "`Submission` is an abstract class that enforces the implementation of a run method for your solution. This is important because we expect your submission to have a method with that name, which accepts a `str` and returns a `str`. This method is how your crew will receive the question (prompt) and how we (the GDSC team) expect to get the answer.\n",
    "\n",
    "Implementation of this class can be found in the [src/static/submission.py](src/static/submission.py)\n",
    "\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Submission(ABC):\n",
    "    @abstractmethod\n",
    "    def run(self, prompt: str) -> str:\n",
    "        ...\n",
    "```\n",
    "\n",
    "As you can see, this file contains nothing more than the abstract class Submission. It serves as a useful interface that enforces your submissions to implement the run method.\n",
    "\n",
    "It's worth mentioning that due to Python's dynamic typing, any object with a method named `run` that satisfies the signature will work. However, this abstract class is a good programming practice because it clearly defines the expected interface for your submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your first submission\n",
    "Let's go back to the `BasicPIRLSCrew` defined in the previous tutorial. We already extracted the code into a Python script. The code can be found in the [src/submission/crews/basic_PIRLS_crew.py](src/submission/crews/basic_PIRLS_crew.py). \n",
    "Note how the `BasicPIRLSCrew` inherits from the `Submission` class and implements the `run` function.\n",
    "\n",
    "```python\n",
    "# ... code cut out\n",
    "\n",
    "from src.static.submission import Submission\n",
    "\n",
    "\n",
    "class PythonHelpCrew(Submission):  # PythonHelpCrew inhertis from Submission class\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def run(self, prompt: str) -> str:\n",
    "        return self.crew().kickoff(inputs={\"prompt\": prompt}).raw\n",
    "\n",
    "# ... code cut out\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the `query_database` tool is now in [src/submission/tools/database.py](src/submission/tools/database.py). Slightly changed to not return output that is too long. \n",
    "(Having the output of a `SELECT * FROM STUDENTS` as input to your LLM is a surefire way to spend all your money and crashing the system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our `BasicPIRLSCrew` class is ready, we can modify the `create_submission` function so that it returned our new class.\n",
    "\n",
    "```python\n",
    "from src.submission.crews.basic_PIRLS_crew import BasicPIRLSCrew\n",
    "from src.static.ChatBedrockWrapper import ChatBedrockWrapper\n",
    "from src.static.submission import Submission\n",
    "\n",
    "def create_submission(call_id: str) -> Submission:\n",
    "    llm = ChatBedrockWrapper(\n",
    "        model_id='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "        model_kwargs={'temperature': 0},\n",
    "        call_id=call_id\n",
    "    )\n",
    "    crew = PythonHelpCrew(llm=llm)  # instantiate the new class\n",
    "    return crew\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important point to note is the `ChatBedrockWrapper` class, which is used as the llm argument for your crew. This class is an extension of the [`ChatBedrock`](https://python.langchain.com/v0.2/docs/integrations/chat/bedrock/) class you used in the previous tutorial, and it handles communication with [AWS Bedrock](https://aws.amazon.com/bedrock/). The main difference is that it requires an additional argument, `call_id`, due to a technical requirement in this year's GDSC. The key point is that you **must** use this class instead of the standard ChatBedrock. This wrapper provides direct access to the number of tokens used by your submission. (See [src/static/ChatBedrockWrapper.py](src/static/ChatBedrockWrapper.py))\n",
    "\n",
    "While you're not required to use crewAI, you **must** use this wrapper for all your llm interactions, as it includes the implementation for token counting and cost tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to test your code?\n",
    "There are two ways to test your code:\n",
    "- In AWS by pushing it to the test branch\n",
    "- On your local machine\n",
    "\n",
    "We go over the AWS method now. The local version is explained in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: How to push to the submission branch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing using AWS\n",
    "\n",
    "First, we need to push our changes to the `test_submission`. \n",
    "\n",
    "[<img src=\"../images/t4_push.png\"  width=1200/>](../images/t4_push.png)\n",
    "\n",
    "This will automatically start a pipeline that will create a docker image containing the code that you just pushed, and start the app using the `app.py` script. You can later go to the [Elastic Contaier Service (ECS)](https://us-east-1.console.aws.amazon.com/ecs/v2/clusters?region=us-east-1), go to the clusters and select the gdsc cluster, select the test service, and go to the taks to see its public IP address.\n",
    "\n",
    "[<img src=\"../images/t4_ecs_1.png\"  width=1200/>](../images/t4_ecs_1.png)\n",
    "\n",
    "[<img src=\"../images/t4_ecs_2.png\" width=1200/>](../images/t4_ecs_2.png)\n",
    "\n",
    "Now that we have the IP of our new submission we can send requests to this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def ask_question(question: str, url: str):\n",
    "    data = {'prompt': question}\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOTE_HOST = 'http://100.24.19.134:8000/run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ask_question(\"How many students participated in the study?\", REMOTE_HOST)\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of values returned by your submission:\n",
    "- result - it is the answer retuend by your crew as a result of calling the `run` method.\n",
    "- time - time it took for your crew to answer the question in seconds.\n",
    "- timed_out - information about whether your submission timed out or not.\n",
    "- tokens - overall number of tokens used in all agents' converstions to generate the final answer.\n",
    "- cost - what was the cost for your submission for getting the answer.\n",
    "- token_details - this is a dictionary that holds more detailed data about token usage. Here you can see how many prompt and completion tokens used each of the models you selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to submit your code?\n",
    "Submitting your code is similar to testing it using AWS. This time, however, you will be pushing your changes to the `submission` branch. Like the `test_submission` process, this initiates the `app.py` script in the Docker instance, but it doesn't stop there. An automatic evaluation is run to test your submission, and you can monitor the evaluation status on your team's page. Aftrer successfull evaluation your submission will be allowed to the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the evaluation work?\n",
    "After submitting your code it is tested using a few automatic evaluation questions. Your submission has to yield correct answers, and the response time should be shorter than the specified timeout. If either of these conditions is not fullfilled for any of the automatic evaluation questions, your submission is not allowed to participate any further. Nonetheless, it still affects the total number of submissions your team has made.\n",
    "\n",
    "We all know how unstable LLMs tend to be. This is why each automatic evaluation question is asked 3 times, and in the worst case scenario, the total evaluation can take up to 1 hour! Be patient and you'll see your results in the team's page.\n",
    "\n",
    "# TODO: \n",
    "- Add team page screenshot where eval is failed\n",
    "- Show how to check the logs\n",
    "- Understand other error codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to check the status of your application?\n",
    "You can access the logs directly on ECS. Go to the test task and enter the currently running application. Click on the 'Logs' located on the top bar. If you want to access the logs in real-time, click on the button to CloudWatch.\n",
    "\n",
    "The first step is the same as [here](#testing-using-aws)\n",
    "\n",
    "[<img src=\"../images/t4_ecs_1.png\" width=1200/>](../images/t4_ecs_1.png)\n",
    "\n",
    "[<img src=\"../images/t4_log_1.png\" width=1200/>](../images/t4_log_1.png)\n",
    "\n",
    "[<img src=\"../images/t4_log_2.png\" width=1200/>](../images/t4_log_2.png)\n",
    "\n",
    "Logs are only enabled for the testing tasks and the ECS task will automatically **shut down after 30 minutes**. This should be enough time to run your tests but if you need more time, you would have to make a dummy change to the code and push your changes to the **test_submission** branch again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do if the automatic evaluation fails?\n",
    "If the automatic evaluation fails that means your new submission did not answered correctly, timed out or other unexpected error occured. The reason is displayed on the the team's website:\n",
    "- \"Unexpected error\" - this status means there were issues with starting the `app.py` script. Chceck your submission implementation and `create_submission` method implementation.\n",
    "- \"Timeout\" - this status means that your submission failed to answer to at least one of automatic evaluation questions in time.\n",
    "- \"Incorrect answers\" - this status means that at least one of the answers was not correct. \n",
    "\n",
    "If the displayed status is \"In progress\" for a suspiciously long period of time, that being a few hours, please contact GDSC organizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Remove this and refer to tut5 on how to make them better\n",
    "\n",
    "# Better agents\n",
    "Although our first application worked, we did not receive the right answer. Our current solution has no connection to the database and cannot answer questions that require some insight into the data. Let's fix that and develop our first crew that can extend its knowledge and retrieve additional information! The code for this new crew can be found in [src/submission/crews/student_knowing_crew.py](../src/submission/crews/student_knowing_crew.py)\n",
    "\n",
    "As in the previous example, here we created an agent that can use a tool - this time it's the `query_database` tool. It allows us to query the database using the [sqlalchemy](https://www.sqlalchemy.org/) engine. Now our crew can access additional information!\n",
    "In order to establish connection to the database we had to include a few more credentials. You've seen those in the very first tutorial regarding the PIRLS data.\n",
    "\n",
    "Now let's try and run our application with this new solution. To do that not only do we need an LLM but also an instance of sqlalchemy engine. This requires additional credentials, allowing our `StudentKnowingCrew` to connect to the databse.\n",
    "\n",
    "Let's implement all the necessary changes inside our `create_submission` function.\n",
    "\n",
    "```python\n",
    "import sqlalchemy\n",
    "from src.submission.crews.student_knowing_crew import StudentKnowingCrew  # import the new crew\n",
    "from src.static.ChatBedrockWrapper import ChatBedrockWrapper\n",
    "from src.static.submission import Submission\n",
    "\n",
    "def create_submission(call_id: str) -> Submission:\n",
    "    llm = ChatBedrockWrapper(\n",
    "        model_id='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "        model_kwargs={'temperature': 0},\n",
    "        call_id=call_id\n",
    "    )\n",
    "    \n",
    "    crew = StudentKnowingCrew(llm)  # instantiate the new crew\n",
    "    return crew\n",
    "\n",
    "```\n",
    "\n",
    "Now let's run our app locally and see if it can answer questions about PIRLS data! ([see section \"How to test your code - testing locally\"](#testing-locally))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\t'result': 'The number of students who participated in the PIRLS study is 367575.'\n",
      "\t'time': 2.8910000000614673\n",
      "\t'timed_out': False\n",
      "\t'tokens': 917\n",
      "\t'cost': 0.0003222500000000003\n",
      "\t'token_details': {'anthropic.claude-3-haiku-20240307-v1:0': {'prompt_tokens': 824, 'completion_tokens': 93}}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res = ask_question(\"How many students participated in the study?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ask_question(\"How many countries participated in the study\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we finally get a bunch of neat answers to a real world questions regarding PIRLS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Arena\n",
    "Evaluating LLM-based solutions is not a trivial task. There are no straightforward metrics, such as accuracy or F1 score, to easily compare different models. This is why most of the time, when dealing with LLM text output, human or semi-automated evaluation is used. This idea of human-based evaluation underlies the concept of the Chatbot Arena.\n",
    "\n",
    "As a **human evaluator**, you can go to the [arena website](TODO:), choose a question from a list, and see how 2 random submissions responded to this question. Compare the results and decide which chatbot returned a better answer. Perhaps both are acceptable, or maybe both are complete nonsense? Below the text areas, select the appropriate verdict.\n",
    "\n",
    "It's worth mentioning that everyone can evaluate questions, not just the people taking part in the GDSC.\n",
    "It is also highly unlikely that you will evaluate the exact same battle twice.\n",
    "\n",
    "[<img src=\"../images/t4_arena_1.png\" width=1200/>](../images/t4_arena_1.png)\n",
    "\n",
    "[<img src=\"../images/t4_arena_2.png\" width=1200/>](../images/t4_arena_2.png)\n",
    "\n",
    "[<img src=\"../images/t4_arena_3.png\" width=1200/>](../images/t4_arena_3.png)\n",
    "\n",
    "#### Why should I rank battles?\n",
    "We need human evaluators to assess the quality of the returned answers. Your decision is counted as a win or loss for the competing submissions, and their ranking is updated based on your opinion. Because this is such an important step, we (the GDSC Team) have introduced some constraints on how many submissions your team can make. The first two submissions require no additional effort. However, if your team wants to add more submissions, it is required to rank a specific number of battles. This number starts low for the initial submissions and increases over time to a steady value of 50 ranked battles per submission.\n",
    "\n",
    "This number is a total for your team and not a requirement for a single team member.\n",
    "\n",
    "This system had to be implemented because having a large number of submissions requires a lot of battles for adequate ranking. Because there is almost no automatic evaluation in this year's GDSC edition, it's best to submit solutions that are robust and have a real chance of competing with others, rather than focusing on minor improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Evaluation Questions\n",
    "Right now, on the arena website, you can see a bunch of predefined questions that can be asked to assess the quality of submitted crews. Every submission that passes the automatic evaluation is asked this set of questions, and the responses are stored in the database. You can, however, submit your own questions - specifically tough ones that your crew implementation excels at! This could help your submission win more battles and climb the rankings, as long as it can also handle the existing questions. We're seeking a general solution, not a highly specialized one.\n",
    "\n",
    "Before adding a new question, be sure there is no similar question already in the list. This list will change over time as new, interesting questions pop up either from the GDSC Team's end or from you and other participants.\n",
    "\n",
    "**All questions added by participants will be verified before being added to the list.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Add conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on a locally\n",
    "\n",
    "If you are not working in AWS Sagemaker, there a few things to consider. Let's go over them.\n",
    "\n",
    "### Checking out the code commit repo\n",
    "If you want to use this code in your local IDE, you'll need to install git-remote-codecommit, set up credentials for your AWS account locally, and then use the HTTPS (GRC) clone URL to download the repo. A tutorial for that can be found [here](https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-git-remote-codecommit.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing locally\n",
    "Run the application locally and send a bunch of test requests to localhost url. To do that run this command in the terminal\n",
    "\n",
    "#### Linux\n",
    "```bash\n",
    "# activate python venv if needed (assuming venv is the virtual enviroment direcotry)\n",
    "source venv/bin/activate\n",
    "\n",
    "# add the current directory to the python path\n",
    "export PYTHONPATH=\"$PYTHONPATH:$(pwd)\"\n",
    "\n",
    "# run the application\n",
    "python src/static/app.py\n",
    "```\n",
    "\n",
    "#### Windows\n",
    "```bat\n",
    "rem activate python venv if needed (assuming venv is the virtual enviroment direcotry)\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "rem add the current direcotry to the python path\n",
    "set PYTHONPATH=%PYTHONPATH%;%cd%\n",
    "\n",
    "rem run the application\n",
    "python src\\static\\app.py\n",
    "```\n",
    "\n",
    "#### Expected console output:\n",
    "```\n",
    "INFO:     Started server process [7956]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "Now we can see that our application runs on the localhost. To test it out we need to send a POST request to the localhost/run endpoint with a payload that contains the prompt. Here is an exaple code how to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ask_question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m LOCAL_HOST \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:8000/run\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mask_question\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many students participated in the study?\u001b[39m\u001b[38;5;124m\"\u001b[39m, LOCAL_HOST)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ask_question' is not defined"
     ]
    }
   ],
   "source": [
    "LOCAL_HOST = 'http://127.0.0.1:8000/run'\n",
    "res = ask_question(\"How many students participated in the study?\", LOCAL_HOST)\n",
    "print(res['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
