{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial 5** - Advanced AI Agents\n",
    "\n",
    "In the previous tutorials, you learned how to create AI agents and, more importantly, how to allow them to access the PIRLS data. However, the basic solution presented in tutorial 4 was not sufficient. In this tutorial, we will show you how to improve your solution, enhance your AI agents, and provide general tips for potential improvements to your crew!\n",
    "\n",
    "# Agenda\n",
    "1. [How to improve?](#how-to-improve) - recalls the data structure and shows how to make improvements to t he agents.\n",
    "2. [Modify your code and submit changes](#modify-your-code-and-submit-changes).\n",
    "3. [Submission status on the website](#submission-status-on-the-website) - shows you how to check the status of yoursubmission during automatic evaluation.\n",
    "4. [When is your ranking updated?](#when-is-your-ranking-updated) - explains the process of ranking change.\n",
    "5. [Ways to improve](#ways-to-improve) - shows general tips on how to improve your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve?\n",
    "Although the `BasicPIRLSCrew` worked for a few questions, it was not sufficient to answer more general questions about PIRLS. This was due to the limited amount of information provided to the agents. To recap, we created this code:\n",
    "\n",
    "```python\n",
    "@agent\n",
    "def database_expert(self) -> Agent:\n",
    "    return Agent(\n",
    "        role=\"Students database expert\",\n",
    "        backstory=dedent(\"\"\"\n",
    "            Database expert that knows the structure of PIRLS database regarding students.\n",
    "            You know that there is the table Students with columns Stident_ID and Country_ID, and a table\n",
    "            Countries with column Country_ID, Name and Code.\n",
    "        \"\"\"),\n",
    "        goal=\"Use the tool to query the database and answer the question.\",\n",
    "        llm=self.llm,\n",
    "        allow_delegation=False,\n",
    "        verbose=True,\n",
    "        tools=[\n",
    "            query_database\n",
    "        ]\n",
    "    )\n",
    "```\n",
    "We can see that our so-called 'database expert' only knows about two tables from our general PIRLS database: Students and Countries, which is a small section of what we actually have. Referring back to the tutorial 2, we know that our database structure looks more like this:\n",
    "\n",
    "[<img src=\"../images/t2_db_schema.png\" width=2000/>](../images/tutorial_2_db_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this clearly shows us how little our current solution understands about the structure of the database. We need a better solution - agents that know much more and can create more advanced queries. Enter the [`AdvancedPIRLSCrew`](../src/submission/crews/advanced_PIRLS_crew.py)!\n",
    "\n",
    "The main difference you’ll notice is that the code is more complex. It now involves 2 agents: the data analyst and the data engineer. Another change is that we now have two static variables that point to 2 configuration YAML files, and the `AdvancedPIRLSCrew` class has a wrapper called `@CrewBase`. These changes allow us to specify configurations for agents and tasks in separate YAML files and help declutter our code.\n",
    "\n",
    "```python\n",
    "#  code cut out ...\n",
    "\n",
    "@CrewBase\n",
    "class AdvancedPIRLSCrew(Submission):\n",
    "    \"\"\"Data Analysis Crew for the GDSC project.\"\"\"\n",
    "    # Load the files from the config directory\n",
    "    agents_config = PROJECT_ROOT / 'submission' / 'config' / 'agents.yaml'\n",
    "    tasks_config = PROJECT_ROOT / 'submission' / 'config' / 'tasks.yaml'\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def run(self, prompt: str) -> str:\n",
    "        return self.crew().kickoff(inputs={'user_question': prompt}).raw\n",
    "\n",
    "#  code cut out ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Now let's take a look at our new agents. The data analyst has no tools to use. They only create results and analyze the data provided by the data engineer. The data engineer, on the other hand, has three tools and extensive knowledge of the database structure. Their task is to provide data that can help answer the question. Let's see what we can find in the  [agents.yaml](../src/submission/config/agents.yaml) file.\n",
    "\n",
    "```yaml\n",
    "lead_data_analyst:\n",
    "  role: >\n",
    "    PIRLS lead data analyst\n",
    "  goal: >\n",
    "    Answer the research questions using the PIRLS 2021 dataset\n",
    "  backstory: >\n",
    "    You are the Lead Data Analyst for the Progress in International Reading Literacy Study (PIRLS) project. \n",
    "    Your expertise in data analysis and interpretation is crucial for providing insights into the dataset.\n",
    "    Your analysis will be used to inform educational policies and practices.\n",
    "    You focus on questions related to reading literacy and educational outcomes!\n",
    "   \n",
    "    While you have a good overview of PIRLS, you always rely on your data engineer to provide you with the necessary data.\n",
    "    When delegating a task to a coworker, remember to explicitly name the coworker you want to delegate to.\n",
    "\n",
    "# file cut out ...\n",
    "    \n",
    "```\n",
    "\n",
    "This file contains the full configuration for both of our new agents (part for the data enginineer cropped out in the above preview). See how extensive the backstory is for the data engineer. It contains basic informations about all the tables in the PIRLS database and even mentions additional information like the possible connections between tables or example queries.\n",
    "\n",
    "It's worth noting that the keys for both `lead_data_analyst` and `data_engineer` are the same as the arguments used for agent initialization.\n",
    "\n",
    "```python\n",
    "Agent(\n",
    "  role=\"...\",\n",
    "  goal=\"...\",\n",
    "  backstory=\"...\"\n",
    ")\n",
    "```\n",
    "\n",
    "Similarly the same holds true for the [tasks.yaml](../src/submission/config/tasks.yaml) file. The keys used in this file must match the keyword arguments used for initializing the `Task` object. One important thing to notice is that all of the config files can be parametrized. Let's take a look at the tasks configuration file.\n",
    "\n",
    "```yaml\n",
    "answer_question_task:\n",
    "  description: >\n",
    "    Answer the following question:    \n",
    "    {user_question}\n",
    "    \n",
    "    When applicable, search for relevant data in the PIRLS 2021 dataset.\n",
    "    \n",
    "    When answering, always:    \n",
    "    - Do not comment on topics outside the area of your expertise.     \n",
    "    - Ensure that your analysis is accurate and relevant to the research questions.\n",
    "    - Unless instructed otherwise, explain how you come to your conclusions and provide evidence to support your claims.\n",
    "    - Use markdown format for your final answer.\n",
    "  expected_output: >\n",
    "    A clear and concise answer to the question\n",
    "```\n",
    "\n",
    "Here we can see that Python string format is being used. In the `description` part we can see the `{user_question}` parameter. How to provide the value for this parameter? You can do that in the `kickoff` method of your crew.\n",
    "\n",
    "```python\n",
    "@CrewBase\n",
    "class DataAnalysisCrew(Submission):\n",
    "    \n",
    "    def run(self, prompt: str) -> str:\n",
    "        return self.crew().kickoff(inputs={'user_question': prompt}).raw\n",
    "\n",
    "# ... code cut out\n",
    "```\n",
    "\n",
    "We can see that in the `run` method when we call the crew's `kickoff` method we pass an argument called `inputs` which is a `dict` that uses the names of the parameters defined in the YAML file as keys. Although we used only one parameter here, you can define more and pass the values in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important thing to pay attention to are the tools given to the data engineer agent. They can be found in the [src/submission/tools/database.py](../src/submission/tools/database.py). You may already be familiar with one of the functions because it was used by our previous `BasicPIRLSCrew`. The other two are quite similar to each other, and their main goal is to reduce the responsibility of generating SQL queries by the AI agents and instead provide them with a straightforward way to use predefined queries. These two tools focus on the questionnaire part of the database. If we take a closer look, we can see that there are a few similar tables, with the main difference being the context of the questions asked in the questionnaires. This can help us abstract some of the queries. Let's take a look at the `get_possible_answers_to_question` tool:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def get_possible_answers_to_question(\n",
    "        general_table: Literal['Students', 'Curricula', 'Homes', 'Teachers', 'Schools'],\n",
    "        questionnaire_answers_table: Literal['StudentQuestionnaireAnswers', 'CurriculumQuestionnaireAnswers', 'HomeQuestionnaireAnswers', 'TeacherQuestionnaireAnswers', 'SchoolQuestionnaireAnswers'],\n",
    "        questionnaire_entries_table: Literal['StudentQuestionnaireEntries', 'CurriculumQuestionnaireEntries', 'HomeQuestionnaireEntries', 'TeacherQuestionnaireEntries', 'SchoolQuestionnaireEntries'],\n",
    "        question_code: str\n",
    ") -> str:\n",
    "    \"\"\"Query the database and returns possible answer to a given question\n",
    "\n",
    "    Args:\n",
    "        general_table (str): the generic table related to the question topic. Can be one of: 'Students', 'Curricula', 'Homes', 'Teachers', 'Schools'\n",
    "        questionnaire_answers_table (str): the table related to the `general_table` containing answers.\n",
    "        questionnaire_entries_table (str): the table related to the `general_table` containing all possible questions.\n",
    "        question_code (str): the code of the question the full list of possible answers to is returned.\n",
    "\n",
    "    Returns:\n",
    "        str: The list of all possible answers to the question with the code given in `question_code`.\n",
    "    \"\"\"\n",
    "    entity_id = 'curriculum_id' if general_table.lower() == 'curricula' else f'{general_table.lower()[:-1]}_id'\n",
    "    query = f\"\"\"\n",
    "        SELECT DISTINCT ATab.Answer\n",
    "        FROM {general_table} AS GTab\n",
    "        JOIN {questionnaire_answers_table} AS ATab ON ATab.{entity_id} = GTab.{entity_id}\n",
    "        JOIN {questionnaire_entries_table} AS ETab ON ETab.Code = ATab.Code\n",
    "        WHERE ATab.Code = '{question_code.replace(\"'\", \"\").replace('\"', '')}'\n",
    "    \"\"\"\n",
    "\n",
    "    with ENGINE.connect() as connection:\n",
    "        try:\n",
    "            res = connection.execute(text(query))\n",
    "        except Exception as e:\n",
    "            return f'Wrong query, encountered exception {e}.'\n",
    "\n",
    "    ret = \"\"\n",
    "    for result in res:\n",
    "        ret += \", \".join(map(str, result)) + \"\\n\"\n",
    "\n",
    "    return ret\n",
    "\n",
    "```\n",
    "\n",
    "This function is well documented. This is important because the agents use the Python docstrings to determine the purpose of the function. Even if some might say this documentation is a bit too verbose, it will surely help the agents understand how they are supposed to use this tool.\n",
    "\n",
    "And what does this tool actually do? It returns a list of possible answers that the participants of the PIRLS had to choose from. For example, the possible answers to the question \"About how many books are there in your home?\" are as follows (question code ASBG04):\n",
    "\n",
    "- nan\n",
    "- Enough to fill two bookcases (101–200 books)\n",
    "- Enough to fill one shelf (11–25 books)\n",
    "- None or very few (0–10 books)\n",
    "- Enough to fill three or more bookcases (more than 200)\n",
    "- Enough to fill one bookcase (26–100 books)\n",
    "- Omitted or invalid\n",
    "\n",
    "To obtain this result the database engineer agent can pass **Students** as the `general_table`, **StudentQuestionnaireAnswers** as the `questionnaire_answers_table`, **StudentQuestionnaireEntries** as the `questionnaire_entries_table`, and **ASBG04** as the `question_code`. That will generate this query:\n",
    "\n",
    "```sql\n",
    "select distinct ATab.Answer\n",
    "from Students as GTab\n",
    "join StudentQuestionnaireAnswers as ATab on ATab.student_id = GTab.student_id\n",
    "join StudentQuestionnaireEntries as ETab on ETab.code = ATab.code\n",
    "where ATab.code = 'ASBG04'\n",
    "\n",
    "```\n",
    "This tool might be useful because of the structure of the database. It has 5 questionnaire-related sections that are quite similar to each other, and generating almost the same SQL query repeatedly could be pointless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify your code and submit changes\n",
    "If you want to make a new submission to the GDSC, all you have to do is push your desired changes to the **submission** branch of your repository. If the branch is not already there, you can create it. Just remember that the names of the branches that trigger the whole AWS pipeline are: **test_submission** for testing purposes, and **submission** for submitting your new solution. We explained how to push changes in details in tutorial 4.\n",
    "\n",
    "Now let's submit our new, better crew! Just modify the `create_submission` function so that it instantiated our advanced crew, and push changes. As stated in the tutorial 4 you can do it by both using JupyterLab console or direclty from CodeCommit.\n",
    "\n",
    "[<img src=\"../images/t5_code_commit_1.png\" width=1200/>](../images/t5_code_commit_1.png)\n",
    "\n",
    "Be sure that you are making changes on the **submission** branch!\n",
    "\n",
    "[<img src=\"../images/t5_code_commit_2.png\" width=1200/>](../images/t5_code_commit_2.png)\n",
    "\n",
    "Fill *author name*, *author eamil* and optionally *commit message* nad press *commit changes*. \n",
    "\n",
    "[<img src=\"../images/t5_code_commit_3.png\" width=1200/>](../images/t5_code_commit_3.png)\n",
    "\n",
    "Now you can go to Elastic Container Service (ECS) and see the running task. Go to the clusters and select the gdsc cluster, select the gdscSubmit service, and go to the taks to see its public IP address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission status on the website\n",
    "We can monitor the submission status on the GDSC website. Firstly a short while after your submission you should see that a new submission was created and its status is \"In progress\".\n",
    "\n",
    "[<img src=\"../images/t4_submission_status_1.png\" width=1000/>](../images/t4_submission_status_1.png)\n",
    "\n",
    "We should wait for a while (just as a reminder the worst case scenario is that the evauation takes up to one hour!) and hopefully, if everything was done correctly we should see that our submission passed the evaluation!\n",
    "\n",
    "[<img src=\"../images/t5_submission_status_2.png\" width=1000/>](../images/t5_submission_status_2.png)\n",
    "\n",
    "That went smoothly! Now our submission is ready to throw its hat in the ring, answer all the arena questions and outshine the competition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard\n",
    "To see the lederboard you have to go to he leaderboard page. My team was called AIcademics and surely you can see this new submission with the default rating value of 1500. Now we can do some battles or wait for others to do some and hopefully the ranking will change.\n",
    "\n",
    "[<img src=\"../images/t5_leaderboard_1.png\" width=1000/>](../images/t5_leaderboard_1.png)\n",
    "\n",
    "After a while you should see what is your current position on the leaderboard, and how much did your ranking change since the last update.\n",
    "\n",
    "[<img src=\"../images/t5_leaderboard_2.png\" width=1000/>](../images/t5_leaderboard_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When is your ranking updated?\n",
    "The underlying elo ranking for your submission is updated after every battle. It is not directly shown to the participants. What you can see is the leaderboard which is updated **once a day**. The difference between days is calculated based on the *current* and *previous* ranking, and they both start at 1500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways to improve\n",
    "There are many potential ways to improve the performance of your agents starting from giving them a broader context and better descriptions, using more agents, developing better tools for accessing the database or even adding a completely new ones such as arithmetic tools or tools that can access the internet.\n",
    "\n",
    "Aditionally you can test how good is your submission in answering the set of questions available on the website. Maybe you can find something that gives your agents a bit of a challenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "The most important takeaway from this tutorial is how to enhance your AI agents. You should now be equipped to modify the agent code, refine the current solution, or even create an entirely new one. The key to success lies in your imagination and creativity!\n",
    "\n",
    "We also demonstrated how to declutter your code by separating the prompt text, maintain a clean code structure, and parametrize your prompts effectively. Additionally, we introduced several tool ideas and emphasized the importance of paying close attention to the database structure and the information provided to the agents. Armed with this knowledge, you should be ready to compete in this year's GDSC and hold your ground in the arena, whether you’re a seasoned data scientist or a novice developer. May thy submissions shine and make their mark!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
